{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SIEHdTfoZm5P",
        "outputId": "b94efc30-34e9-4a43-86a6-c572336ede5c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original Attention Scores (wei):\n",
            " tensor([[-0.9529,  0.0129, -0.1987,  0.5916],\n",
            "        [ 2.0206, -0.0556, -1.2152,  0.4703],\n",
            "        [-0.6061, -1.1618, -0.6499,  0.3146],\n",
            "        [ 0.4922,  0.2849, -1.0224, -0.6005]])\n",
            "\n",
            "Lower-triangular Mask (tril):\n",
            " tensor([[1., 0., 0., 0.],\n",
            "        [1., 1., 0., 0.],\n",
            "        [1., 1., 1., 0.],\n",
            "        [1., 1., 1., 1.]])\n",
            "\n",
            "Masked Attention Scores (wei after masking):\n",
            " tensor([[-0.9529,    -inf,    -inf,    -inf],\n",
            "        [ 2.0206, -0.0556,    -inf,    -inf],\n",
            "        [-0.6061, -1.1618, -0.6499,    -inf],\n",
            "        [ 0.4922,  0.2849, -1.0224, -0.6005]])\n",
            "\n",
            "Normalized Attention Weights (after softmax):\n",
            " tensor([[1.0000, 0.0000, 0.0000, 0.0000],\n",
            "        [0.8886, 0.1114, 0.0000, 0.0000],\n",
            "        [0.3951, 0.2267, 0.3782, 0.0000],\n",
            "        [0.4223, 0.3432, 0.0929, 0.1416]])\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Sequence length T\n",
        "T = 4\n",
        "\n",
        "# Random tensor representing \"attention scores\"\n",
        "wei = torch.randn(T, T)\n",
        "print(\"Original Attention Scores (wei):\\n\", wei)\n",
        "\n",
        "# Generate a lower-triangular matrix (ones below diagonal, zeros above)\n",
        "tril = torch.tril(torch.ones(T, T))\n",
        "print(\"\\nLower-triangular Mask (tril):\\n\", tril)\n",
        "\n",
        "# Apply the masking: set upper-triangular entries to -inf\n",
        "wei_masked = wei.masked_fill(tril == 0, float('-inf'))\n",
        "# read it as follows : For every position where the corresponding entry in tril is zero, set the value at the same position in wei to negative infinity (-inf).\n",
        "\n",
        "print(\"\\nMasked Attention Scores (wei after masking):\\n\", wei_masked)\n",
        "\n",
        "# Applying softmax along the rows to get normalized attention\n",
        "attention_weights = torch.softmax(wei_masked, dim=-1)\n",
        "print(\"\\nNormalized Attention Weights (after softmax):\\n\", attention_weights)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HJcLsKo3ZnY3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}