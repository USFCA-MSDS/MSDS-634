{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## ðŸš— Deep Double Q-Learning on MountainCar-v0\n",
        "\n",
        "This project implements a clean and minimal version of **Deep Double Q-Learning (Double DQN)** to solve the `MountainCar-v0` environment from OpenAI Gym. The goal is to teach an agent to drive a car up a steep hill using reinforcement learning.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§  Background: Q-Learning vs. Double Q-Learning\n",
        "\n",
        "In **standard Q-learning**, the agent learns the value of state-action pairs $$Q(s, a)$$ using the Bellman update:\n",
        "\n",
        "$$\n",
        "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\left[ r + \\gamma \\max_{a'} Q(s', a') - Q(s, a) \\right]\n",
        "$$\n",
        "\n",
        "However, this can **overestimate** action values because the same Q-function is used for both selecting and evaluating the next action.\n",
        "\n",
        "---\n",
        "\n",
        "### â— Double Q-Learning Fixes This\n",
        "\n",
        "**Double Q-Learning** introduces two Q-networks:\n",
        "- A **policy network** $$Q_\\theta$$ to select actions.\n",
        "- A **target network** $$Q_{\\theta^-}$$ to evaluate the selected actions.\n",
        "\n",
        "The updated target becomes:\n",
        "\n",
        "$$\n",
        "y = r + \\gamma Q_{\\theta^-}(s', \\arg\\max_{a'} Q_\\theta(s', a'))\n",
        "$$\n",
        "\n",
        "Then the loss function becomes:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\left( Q_\\theta(s, a) - y \\right)^2\n",
        "$$\n",
        "\n",
        "This reduces overestimation by **decoupling** selection and evaluation.\n",
        "\n",
        "---\n",
        "\n",
        "## ðŸ“ Environment: `MountainCar-v0`\n",
        "\n",
        "- **State**: A continuous 2D vector $$(\\text{position}, \\text{velocity})$$\n",
        "- **Action Space**: {0: Push Left, 1: No Push, 2: Push Right}\n",
        "- **Reward**: -1 per step until the car reaches the goal at position $$\\geq 0.5$$\n",
        "\n",
        "This environment is challenging due to:\n",
        "- **Sparse rewards** (no feedback until the goal)\n",
        "- **Delayed credit assignment** (the agent must first move backward to gain momentum)\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ§® Q-Network Architecture\n",
        "\n",
        "Each Q-network is a simple MLP:\n",
        "\n",
        "$$\n",
        "\\text{Input: } s = [\\text{position}, \\text{velocity}] \\in \\mathbb{R}^2 \\\\\n",
        "\\text{Network: } \\text{Linear}(2 \\to 128) \\rightarrow \\text{ReLU} \\rightarrow \\text{Linear}(128 \\to 3)\n",
        "$$\n",
        "\n",
        "The output is a vector $$Q(s, \\cdot) \\in \\mathbb{R}^3$$ with one value per action.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ” Training Loop Summary\n",
        "\n",
        "1. Initialize environment and agent.\n",
        "2. At each step:\n",
        "   - Choose action using **epsilon-greedy** strategy.\n",
        "   - Store the transition $$(s, a, r, s', \\text{done})$$ in a **replay buffer**.\n",
        "3. Sample a batch of transitions and compute the target:\n",
        "   - Use `policy_net` to select the next action.\n",
        "   - Use `target_net` to evaluate the value of that action.\n",
        "4. Compute the loss and update the `policy_net`.\n",
        "5. Periodically update `target_net` using `policy_net` weights.\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ”§ Core Components\n",
        "\n",
        "| Component         | Description |\n",
        "|------------------|-------------|\n",
        "| `policy_net`      | Learns the Q-function. Used for action selection. |\n",
        "| `target_net`      | Provides stable Q-targets. Updated slowly. |\n",
        "| Replay Buffer     | Stores experience tuples for training. |\n",
        "| Epsilon-Greedy    | Balances exploration (random) and exploitation (greedy). |\n",
        "| Target Update     | Synchronizes target weights every few episodes. |\n",
        "\n",
        "---\n",
        "\n",
        "### ðŸ“‰ Loss Function\n",
        "\n",
        "For a batch of transitions:\n",
        "\n",
        "$$\n",
        "\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^{N} \\left( Q_\\theta(s_i, a_i) - \\left[ r_i + \\gamma Q_{\\theta^-}(s'_i, \\arg\\max_{a'} Q_\\theta(s'_i, a')) \\right] \\right)^2\n",
        "$$\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "w6bn73anBnh5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "u0q5ymTMBmuc"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------\n",
        "# Deep Double Q-Learning for OpenAI Gym's MountainCar-v0\n",
        "# ------------------------------------------------------------\n",
        "# This is a self-contained, fully annotated implementation that:\n",
        "#   - Uses two neural networks for Double DQN (policy and target)\n",
        "#   - Trains using experience replay\n",
        "#   - Applies epsilon-greedy exploration\n",
        "#   - Reduces overestimation by decoupling action selection & evaluation\n",
        "# ------------------------------------------------------------\n",
        "\n",
        "import gym\n",
        "import numpy as np\n",
        "import random\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# -------------------------------\n",
        "# SETUP\n",
        "# -------------------------------\n",
        "# Fix seeds for reproducibility\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "# Use GPU if available for faster training\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Initialize environment\n",
        "env = gym.make('MountainCar-v0')\n",
        "state_dim = env.observation_space.shape[0]  # 2D state: [position, velocity]\n",
        "n_actions = env.action_space.n              # 3 possible actions: left, neutral, right\n",
        "\n",
        "# -------------------------------\n",
        "# Q-NETWORK DEFINITION\n",
        "# -------------------------------\n",
        "class QNetwork(nn.Module):\n",
        "    \"\"\"\n",
        "    A simple feedforward neural network for approximating Q-values.\n",
        "    Given a state, it outputs Q-values for all possible actions.\n",
        "    \"\"\"\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(state_dim, 128),  # Input layer: 2 -> 128\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(128, n_actions)   # Output layer: 128 -> 3 (Q-values)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "# -------------------------------\n",
        "# HYPERPARAMETERS\n",
        "# -------------------------------\n",
        "episodes = 500               # Total training episodes\n",
        "gamma = 0.99                 # Discount factor\n",
        "epsilon = 1.0                # Initial exploration rate\n",
        "epsilon_min = 0.01           # Minimum epsilon after decay\n",
        "epsilon_decay = 0.995        # Decay rate per episode\n",
        "lr = 1e-3                    # Learning rate\n",
        "batch_size = 64              # Mini-batch size for updates\n",
        "memory_size = 10_000         # Max replay buffer size\n",
        "target_update_freq = 10      # Update target network every N episodes\n",
        "\n",
        "# -------------------------------\n",
        "# MEMORY (Replay Buffer)\n",
        "# -------------------------------\n",
        "# Stores experience tuples: (state, action, reward, next_state, done)\n",
        "replay_buffer = deque(maxlen=memory_size)\n",
        "\n",
        "# -------------------------------\n",
        "# INITIALIZE NETWORKS\n",
        "# -------------------------------\n",
        "policy_net = QNetwork().to(device)         # Main Q-network\n",
        "target_net = QNetwork().to(device)         # Target network\n",
        "target_net.load_state_dict(policy_net.state_dict())  # Copy weights initially\n",
        "target_net.eval()  # We donâ€™t train target_net directly\n",
        "\n",
        "# Optimizer and loss function\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=lr)\n",
        "loss_fn = nn.MSELoss()\n",
        "\n",
        "# For tracking episode rewards\n",
        "reward_log = []\n",
        "\n",
        "# -------------------------------\n",
        "# TRAINING LOOP\n",
        "# -------------------------------\n",
        "for ep in range(episodes):\n",
        "    state = env.reset()\n",
        "    total_reward = 0\n",
        "    done = False\n",
        "\n",
        "    while not done:\n",
        "        # Convert state to tensor\n",
        "        state_tensor = torch.FloatTensor(state).unsqueeze(0).to(device)\n",
        "\n",
        "        # Epsilon-greedy action selection\n",
        "        if random.random() < epsilon:\n",
        "            action = env.action_space.sample()  # Explore\n",
        "        else:\n",
        "            with torch.no_grad():\n",
        "                q_vals = policy_net(state_tensor)\n",
        "                action = q_vals.argmax().item()  # Exploit\n",
        "\n",
        "        # Take action in the environment\n",
        "        next_state, reward, done, _ = env.step(action)\n",
        "        total_reward += reward\n",
        "\n",
        "        # Store experience in replay buffer\n",
        "        replay_buffer.append((state, action, reward, next_state, done))\n",
        "        state = next_state  # Move to next state\n",
        "\n",
        "        # Start training when we have enough samples\n",
        "        if len(replay_buffer) >= batch_size:\n",
        "            # Sample a random minibatch\n",
        "            batch = random.sample(replay_buffer, batch_size)\n",
        "            s, a, r, s2, d = zip(*batch)\n",
        "\n",
        "            # Convert to tensors\n",
        "            s = torch.FloatTensor(s).to(device)\n",
        "            a = torch.LongTensor(a).unsqueeze(1).to(device)\n",
        "            r = torch.FloatTensor(r).unsqueeze(1).to(device)\n",
        "            s2 = torch.FloatTensor(s2).to(device)\n",
        "            d = torch.FloatTensor(d).unsqueeze(1).to(device)\n",
        "\n",
        "            # -------------------------------\n",
        "            # DOUBLE DQN TARGET CALCULATION\n",
        "            # -------------------------------\n",
        "            with torch.no_grad():\n",
        "                # Action selection: use policy_net to get best actions\n",
        "                best_actions = policy_net(s2).argmax(dim=1, keepdim=True)\n",
        "                # Action evaluation: use target_net to evaluate chosen actions\n",
        "                target_q = target_net(s2).gather(1, best_actions)\n",
        "                # Compute target: r + Î³ * Q_target(s', a*)\n",
        "                y = r + gamma * target_q * (1 - d)\n",
        "\n",
        "            # Current Q-values for taken actions\n",
        "            q = policy_net(s).gather(1, a)\n",
        "\n",
        "            # Compute MSE loss and backpropagate\n",
        "            loss = loss_fn(q, y)\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # Track rewards for visualization\n",
        "    reward_log.append(total_reward)\n",
        "\n",
        "    # Decay epsilon for less exploration over time\n",
        "    epsilon = max(epsilon * epsilon_decay, epsilon_min)\n",
        "\n",
        "    # Sync target network every few episodes\n",
        "    if (ep + 1) % target_update_freq == 0:\n",
        "        target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "    # Log training progress every 10 episodes\n",
        "    if (ep + 1) % 10 == 0:\n",
        "        avg = np.mean(reward_log[-10:])\n",
        "        print(f\"Episode {ep+1}, Avg Reward: {avg:.2f}, Epsilon: {epsilon:.3f}\")\n",
        "\n",
        "# -------------------------------\n",
        "# VISUALIZE RESULTS\n",
        "# -------------------------------\n",
        "plt.plot(reward_log)\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total Reward\")\n",
        "plt.title(\"Deep Double Q-Learning on MountainCar-v0\")\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "env.close()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "9Y8WBdXcCc-i"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}